#!/usr/bin/env python3
"""
This is the main entry point for Axolotl's command line applications

example usage:

- axolotl batch antismash "/home/user/genome_files/*" "/home/user/bgcs/"
- axolotl help
- axolotl help batch
"""

from pyspark.sql import SparkSession
from sys import argv
from axolotl.batch import get_module_data
from glob import glob
from typing import List
import os


def main() -> int:

    if len(argv) > 1:
        mode = argv[1]
        params = []
        if len(argv) > 2:
            params = argv[2:]

        match mode:
            case "help":
                if len(argv) > 2:
                    help_selected_mode = argv[2]
                else:
                    help_selected_mode = None
                return show_help(help_selected_mode)
            case "run":
                return run_batch_pipeline(params)
            case _:
                print("command not found '%s'!" % (mode,))
                return show_help()
    else:
        return show_help()


def show_help(mode:str=None) -> int:
    print("help_text %s" % (mode, ))


def run_batch_pipeline(params:List[str]) -> int:

    if len(params) < 1:
        show_help("run")
        return 0

    # check if runner exists
    runner_name = params[0]
    runner = get_module_data(params[0])
    if not runner:
        print("[run] runner not found for '%s'!" % (params[0],))
        return 1

    # check if calling help
    if len(params) > 1 and params[1] == "help":
        runner.show_help()
        return 0
    elif len(params) < 3:
        show_help("run")
        return 0

    # solve file paths and construct variables
    input_file_patterns = params[1].split(",")
    output_root_dir = params[2]
    runner_params = []
    if len(params) > 3:
        runner_params = params[3:]

    input_file_paths = set()
    for pattern in input_file_patterns:
        input_file_paths.update(glob(pattern))
    if len(input_file_paths) < 1:
        print("[run] can't find any file matching the input patterns!")
        return 1

    # run module in parallel
    spark_master_ip = os.environ.get("AXOLOTL_RUNNER", "local[*]")
    spark = SparkSession.builder.master(spark_master_ip)
    spark_config_file = os.environ.get("AXOLOTL_RUNNER_CONFIG", None)
    if spark_config_file:
        with open(spark_config_file, "r") as ii:
            for line in ii:
                line = line.rstrip("\n")
                if line != "":
                    key, val = line.split("\t")
                    spark = spark.config(key, val)

    spark = spark.getOrCreate()
    sc = spark.sparkContext
    results = sc.parallelize(input_file_paths, numSlices=len(input_file_paths)).map(lambda x: runner.run_module(x, output_root_dir, runner_params))

    # call
    row_combiner = (lambda x, y: (x[0]+1 if y==-1 else x[0], x[1]+1 if y==0 else x[1], x[2]+1 if y==1 else x[2]))
    part_combiner = (lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))
    results = results.aggregate((0, 0, 0), row_combiner, part_combiner)
    results = {
        "skipped": results[0],
        "failed": results[1],
        "success": results[2]
    }
    print("[run] run results: %s" % results)
    return 0


if __name__ == "__main__":
    return_code = main()